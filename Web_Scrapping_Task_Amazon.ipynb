{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ3zNPXBWmQaODHID5VTos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-with-shobhit/Misc/blob/main/Web_Scrapping_Task_Amazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  ***REQUIREMENTS***  ***(NOTE: RUN ONLY ONE TIME BEFORE SCRAPPER)***"
      ],
      "metadata": {
        "id": "hPqk2RagZEH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess; print(\"Requirements Successful\") if subprocess.run(\"pip install requests beautifulsoup4 pandas fake_useragent\", shell=True).returncode == 0 else None\n"
      ],
      "metadata": {
        "id": "mGauSQQ4ZDg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***WEB SCRAPPER***\n",
        "\n"
      ],
      "metadata": {
        "id": "dm6Nsat9TUkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all neccesary libraries\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "# Using headers to mimic real browser to avoid getting banned / restricted\n",
        "\n",
        "ua = UserAgent()\n",
        "headers = {\n",
        "    'User-Agent': ua.random,\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "\n",
        "\n",
        "# Made a function to check response codes\n",
        "\n",
        "status_codes = status_codes = {100: \"Continue\", 101: \"Switching Protocols\", 102: \"Processing\",\n",
        "                               200: \"OK\", 201: \"Created\", 202: \"Accepted\", 204: \"No Content\",\n",
        "                               301: \"Moved Permanently\", 302: \"Found (Temporary Redirect)\", 304: \"Not Modified\",\n",
        "                               400: \"Bad Request\", 401: \"Unauthorized\", 403: \"Forbidden\", 404: \"Not Found\", 429: \"Too Many Requests\",\n",
        "                               500: \"Internal Server Error\", 502: \"Bad Gateway\", 503: \"Service Unavailable\", 504: \"Gateway Timeout\"}\n",
        "\n",
        "def fetch_url(url):\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "  code=response.status_code\n",
        "  if code in status_codes:\n",
        "      print(\"*\"*100)\n",
        "      print(f\"Response Code: {code} - {status_codes[code]}\")\n",
        "  else:\n",
        "      print(\"*\"*100)\n",
        "      print(f\"Response Code: {code} - Unknown Status\")\n",
        "\n",
        "\n",
        "# Define product category or name of product in this case we are using for laptop/laptops\n",
        "\n",
        "search_product = \"laptops\"                                              # Change for different Brands eg . HP laptops , Dell laptops\n",
        "base_url = f\"https://www.amazon.in/s?k={search_product}\"\n",
        "fetch_url(base_url)\n",
        "print()\n",
        "print(\"*\"*100)\n",
        "\n",
        "\n",
        "# Function to extract product details\n",
        "def get_laptop_data(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    laptops = []\n",
        "    results = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
        "\n",
        "    for result in results:\n",
        "        try:\n",
        "            title = result.find(\"h2\", class_=\"a-size-medium a-spacing-none a-color-base a-text-normal\").text.strip()\n",
        "        except AttributeError:\n",
        "            title = None\n",
        "\n",
        "        try:\n",
        "            image = result.find(\"img\", class_=\"s-image\")[\"src\"]\n",
        "        except (AttributeError, TypeError):\n",
        "            image = None\n",
        "\n",
        "        try:\n",
        "            rating = result.find(\"span\", class_=\"a-icon-alt\").text.strip()\n",
        "        except AttributeError:\n",
        "            rating = None\n",
        "\n",
        "        try:\n",
        "            price = result.find(\"span\", class_=\"a-price-whole\").text.strip()\n",
        "        except AttributeError:\n",
        "            price = None\n",
        "\n",
        "\n",
        "\n",
        "        ad_badge = next((result.find(\"span\", class_=cls)\n",
        "        for cls in [\"puis-label-popover-default\", \"puis-label-popover-hover\",\"a-color-base\", \"a-link-normal aok-inline-block s-widget-sponsored-label-text\"]\n",
        "                         if result.find(\"span\", class_=cls)), result.find(\"span\", string=\"Sponsored\"))\n",
        "\n",
        "        ad_or_organic = \"Ad\" if ad_badge else \"Organic\"\n",
        "\n",
        "        laptops.append({\n",
        "            \"Title\": title,\n",
        "            \"Image\": image,\n",
        "            \"Rating\": rating,\n",
        "            \"Price\": price,\n",
        "            \"Ad/Organic\": ad_or_organic\n",
        "        })\n",
        "\n",
        "    return laptops\n",
        "\n",
        "\n",
        "all_data = []\n",
        "num_pages = 5 # Change this to scrape more pages minimum 5\n",
        "\n",
        "for page in range(1, num_pages + 1):\n",
        "    print(f\"Scraping page ..... {page}\")\n",
        "\n",
        "    url = base_url + str(page)\n",
        "    laptops = get_laptop_data(url)\n",
        "    all_data.extend(laptops)\n",
        "    time.sleep(2)  # add delay in seconds to avoid getting banned\n",
        "\n",
        "print()\n",
        "print(\"*\"*100)\n",
        "\n",
        "# Saving data to CSV with timestamp\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"amazon_laptops_{timestamp}.csv\"\n",
        "df = pd.DataFrame(all_data)\n",
        "df.dropna(inplace=True)                  # Dropping all nan values\n",
        "df.reset_index(drop=True, inplace=True)   # Resetting index to avoid Errors\n",
        "\n",
        "df.to_csv(filename, index=False) # saving file to csv format\n",
        "\n",
        "print(f\"Data saved to {filename}\")\n",
        "print()\n",
        "print(\"Task Completed Successfully!!\")\n",
        "print()\n",
        "print(\"*\"*100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Qo7hC9DS9_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}